"""
DeXposure æ—¶åºå›¾æ•°æ®åŠ è½½å™¨ - ç”¨äºä¼ æŸ“æ•ˆåº”é¢„æµ‹
æ”¯æŒå›å½’ä»»åŠ¡ï¼šé¢„æµ‹èŠ‚ç‚¹åœ¨å±æœºä¸­çš„å—å½±å“ç¨‹åº¦
"""

import json
import pandas as pd
import dgl
import torch
import numpy as np
import subprocess
import sys
from pathlib import Path
from typing import List, Tuple, Dict, Optional


class DeXposureTemporalLoader:
    """
    DeXposure æ—¶åºæ•°æ®åŠ è½½å™¨

    ç”¨é€”ï¼š
    1. åŠ è½½å†å²ç½‘ç»œå¿«ç…§åºåˆ—
    2. æ„å»ºæ—¶åºå›¾å¯¹
    3. æ”¯æŒæ‰‹åŠ¨æ³¨å…¥"èŠ‚ç‚¹ç ´äº§"åœºæ™¯
    4. é¢„æµ‹ä¼ æŸ“æ•ˆåº”ï¼ˆå›å½’ä»»åŠ¡ï¼‰
    """

    def __init__(
        self,
        data_path: str,
        meta_path: str,
        prediction_window: int = 1,  # é¢„æµ‹æœªæ¥å‡ å‘¨
        auto_download: bool = True  # è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†
    ):
        """
        Args:
            data_path: historical-network_week_xxx.json è·¯å¾„
            meta_path: meta_df.csv è·¯å¾„
            prediction_window: é¢„æµ‹çª—å£ï¼ˆé»˜è®¤1å‘¨ï¼‰
            auto_download: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æ—¶æ˜¯å¦è‡ªåŠ¨ä¸‹è½½
        """
        self.data_path = Path(data_path)
        self.meta_path = Path(meta_path)
        self.prediction_window = prediction_window

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self._check_data_files(auto_download)

        # åŠ è½½æ•°æ®
        print(f"åŠ è½½æ•°æ®: {self.data_path}")
        with open(self.data_path) as f:
            self.network_data = json.load(f)['data']

        # åŠ è½½å…ƒæ•°æ®ï¼ˆå°†IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥åŒ¹é…èŠ‚ç‚¹IDï¼‰
        self.meta_df = pd.read_csv(self.meta_path)
        self.meta_df['id'] = self.meta_df['id'].astype(str)
        self.meta_dict = self.meta_df.set_index('id').to_dict('index')

        self.dates = sorted(self.network_data.keys())
        print(f"âœ“ åŠ è½½äº† {len(self.dates)} ä¸ªæ—¶é—´å¿«ç…§")
        print(f"  æ—¥æœŸèŒƒå›´: {self.dates[0]} â†’ {self.dates[-1]}")

    def _build_node_features(self, node: Dict) -> np.ndarray:
        """
        æ„å»ºèŠ‚ç‚¹ç‰¹å¾å‘é‡ï¼ˆä¿®å¤ NaN é—®é¢˜ï¼‰

        ç‰¹å¾åŒ…æ‹¬ï¼š
        1. èŠ‚ç‚¹æ€»è§„æ¨¡ (log scale)
        2. èµ„äº§ç§ç±»æ•°é‡
        3. èµ„äº§å¤šæ ·æ€§ (ç†µ)
        4. æœ€å¤§å•ä¸€èµ„äº§å æ¯”
        5. èŠ‚ç‚¹ç±»åˆ« (one-hot: CEX, DEX, Lending, etc.)
        """
        node_id = node['id']
        size = node['size']
        comp = node['composition']

        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾ï¼ˆæ·»åŠ æ•°å€¼ç¨³å®šæ€§ï¼‰
        log_size = np.log1p(max(size, 0))  # ç¡®ä¿éè´Ÿ
        num_assets = len(comp)

        # èµ„äº§å¤šæ ·æ€§ï¼ˆShannonç†µï¼‰- ä¿®å¤ NaN é—®é¢˜
        diversity = 0.0
        max_concentration = 0.0
        
        if num_assets > 0:
            values = np.array(list(comp.values()))
            # è¿‡æ»¤è´Ÿå€¼å’Œ NaN
            values = np.maximum(values, 0)
            values = np.nan_to_num(values, 0.0)
            
            total = values.sum()
            if total > 1e-10:  # é¿å…é™¤ä»¥æ¥è¿‘é›¶çš„æ•°
                proportions = values / total
                # ä½¿ç”¨æ›´ç¨³å®šçš„ç†µè®¡ç®—
                # é¿å… log(0) äº§ç”Ÿ -inf
                proportions_safe = np.maximum(proportions, 1e-10)
                diversity = -np.sum(proportions * np.log(proportions_safe))
                # è£å‰ªåˆ°åˆç†èŒƒå›´
                diversity = np.clip(diversity, 0, 10)
                max_concentration = proportions.max()

        # ç±»åˆ«ç‰¹å¾ (ä» meta_df è·å–)
        category = self.meta_dict.get(str(node_id), {}).get('category', 'Unknown')
        category_features = self._encode_category(category)

        features = np.array([
            log_size,
            num_assets,
            diversity,
            max_concentration,
            *category_features
        ], dtype=np.float32)

        # æœ€åæ£€æŸ¥ï¼šç§»é™¤ NaN å’Œ Inf
        features = np.nan_to_num(features, nan=0.0, posinf=100.0, neginf=-100.0)

        return features

    def _encode_category(self, category: str) -> List[float]:
        """ç¼–ç åè®®ç±»åˆ«ä¸º one-hot"""
        categories = ['CEX', 'Lending', 'Liquid Staking', 'DEX', 'Bridge',
                     'CDP', 'Restaking', 'Chain', 'Unknown']
        one_hot = [1.0 if cat == category else 0.0 for cat in categories]
        return one_hot

    def get_temporal_pair(
        self,
        t: int,
        inject_failure: Optional[List[str]] = None,
        failure_ratio: float = 0.9  # ç ´äº§èŠ‚ç‚¹ä¿ç•™10%èµ„äº§
    ) -> Tuple[dgl.DGLGraph, dgl.DGLGraph, torch.Tensor]:
        """
        è·å–æ—¶åºå›¾å¯¹ï¼š(t æ—¶åˆ»å›¾, t+1 æ—¶åˆ»å›¾, å˜åŒ–æ ‡ç­¾)

        Args:
            t: æ—¶é—´ç´¢å¼•
            inject_failure: è¦æ³¨å…¥ç ´äº§çš„èŠ‚ç‚¹IDåˆ—è¡¨ï¼ˆä¾‹å¦‚ ['2269'] ä»£è¡¨ Binanceï¼‰
            failure_ratio: ç ´äº§å¯¼è‡´çš„èµ„äº§æŸå¤±æ¯”ä¾‹ï¼ˆé»˜è®¤90%ï¼‰

        Returns:
            graph_t: t æ—¶åˆ»çš„å›¾
            graph_t1: t+1 æ—¶åˆ»çš„å›¾ï¼ˆçœŸå®å€¼ï¼Œç”¨äºéªŒè¯ï¼‰
            labels: æ¯ä¸ªèŠ‚ç‚¹çš„å˜åŒ–æ ‡ç­¾ï¼ˆå›å½’ç›®æ ‡ï¼‰
                - shape: (num_nodes, 3)
                - [:, 0]: TVL å˜åŒ–ç‡ (delta_size / size_t)
                - [:, 1]: ç»å¯¹æŸå¤±é‡‘é¢ (log scale)
                - [:, 2]: å—å½±å“ç¨‹åº¦ [0, 1]
        """
        if t + self.prediction_window >= len(self.dates):
            raise ValueError(f"æ—¶é—´ç´¢å¼•è¶…å‡ºèŒƒå›´: {t} + {self.prediction_window} >= {len(self.dates)}")

        date_t = self.dates[t]
        date_t1 = self.dates[t + self.prediction_window]

        snapshot_t = self.network_data[date_t]
        snapshot_t1 = self.network_data[date_t1]

        # è·å–ä¸¤ä¸ªæ—¶é—´ç‚¹çš„æ‰€æœ‰èŠ‚ç‚¹IDï¼ˆå–äº¤é›†ï¼Œç¡®ä¿ä¸€è‡´ï¼‰
        nodes_t = snapshot_t['nodes']
        nodes_t1 = snapshot_t1['nodes']

        # è¿‡æ»¤æ‰ None çš„èŠ‚ç‚¹ID
        node_ids_t = set(n['id'] for n in nodes_t if n['id'] is not None)
        node_ids_t1 = set(n['id'] for n in nodes_t1 if n['id'] is not None)

        # ä½¿ç”¨ä¸¤ä¸ªæ—¶é—´ç‚¹å…±åŒçš„èŠ‚ç‚¹ï¼Œå¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        common_node_ids = sorted(str(nid) for nid in (node_ids_t & node_ids_t1))

        if len(common_node_ids) < len(node_ids_t) * 0.9:
            print(f"  âš ï¸  è­¦å‘Š: åªæœ‰ {len(common_node_ids)}/{len(node_ids_t)} ä¸ªèŠ‚ç‚¹åœ¨ä¸¤ä¸ªæ—¶é—´ç‚¹éƒ½å­˜åœ¨")

        # æ„å»º t æ—¶åˆ»çš„å›¾ï¼ˆåªåŒ…å«å…±åŒèŠ‚ç‚¹ï¼‰
        graph_t, node_ids_list, sizes_t = self._build_graph_snapshot(
            snapshot_t,
            common_node_ids=common_node_ids,
            inject_failure=inject_failure,
            failure_ratio=failure_ratio
        )

        # æ„å»º t+1 æ—¶åˆ»çš„å›¾ï¼ˆåªåŒ…å«å…±åŒèŠ‚ç‚¹ï¼‰
        graph_t1, _, sizes_t1 = self._build_graph_snapshot(
            snapshot_t1,
            common_node_ids=common_node_ids,
            inject_failure=None  # t+1 ä¸æ³¨å…¥ç ´äº§
        )

        # è®¡ç®—å˜åŒ–æ ‡ç­¾ï¼ˆå›å½’ç›®æ ‡ï¼‰
        labels = self._compute_labels(sizes_t, sizes_t1)

        return graph_t, graph_t1, labels, node_ids_list

    def _build_graph_snapshot(
        self,
        snapshot: Dict,
        common_node_ids: Optional[List[str]] = None,
        inject_failure: Optional[List[str]] = None,
        failure_ratio: float = 0.9
    ) -> Tuple[dgl.DGLGraph, List[str], np.ndarray]:
        """æ„å»ºå•ä¸ªæ—¶é—´ç‚¹çš„å›¾å¿«ç…§"""
        nodes = snapshot['nodes']
        links = snapshot['links']

        # å¦‚æœæŒ‡å®šäº† common_node_idsï¼Œåªä¿ç•™è¿™äº›èŠ‚ç‚¹
        if common_node_ids is not None:
            node_id_set = set(common_node_ids)
            nodes = [n for n in nodes if n['id'] is not None and str(n['id']) in node_id_set]
            # æŒ‰ç…§ common_node_ids çš„é¡ºåºæ’åº
            node_id_to_node = {str(n['id']): n for n in nodes}
            nodes = [node_id_to_node[nid] for nid in common_node_ids if nid in node_id_to_node]

        # èŠ‚ç‚¹IDåˆ—è¡¨ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
        node_ids = [str(n['id']) for n in nodes if n['id'] is not None]
        id_to_idx = {nid: idx for idx, nid in enumerate(node_ids)}

        # æ³¨å…¥ç ´äº§åœºæ™¯
        if inject_failure:
            nodes = self._inject_node_failure(nodes, inject_failure, failure_ratio)

        # æå–èŠ‚ç‚¹ç‰¹å¾å’Œè§„æ¨¡
        node_features = []
        node_sizes = []

        for node in nodes:
            features = self._build_node_features(node)
            node_features.append(features)
            node_sizes.append(node['size'])

        # æ„å»ºè¾¹
        src_nodes = []
        dst_nodes = []
        edge_weights = []

        for link in links:
            source = link['source']
            target = link['target']

            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ£€æŸ¥
            if source is None or target is None:
                continue

            source_str = str(source)
            target_str = str(target)

            if source_str not in id_to_idx or target_str not in id_to_idx:
                continue

            src_idx = id_to_idx[source_str]
            dst_idx = id_to_idx[target_str]
            weight = link['size']

            src_nodes.append(src_idx)
            dst_nodes.append(dst_idx)
            edge_weights.append(weight)

        # åˆ›å»ºåŒå‘å›¾ï¼ˆæ— å‘å›¾ï¼‰
        graph = dgl.graph(
            (src_nodes + dst_nodes, dst_nodes + src_nodes),
            num_nodes=len(nodes)
        )

        # æ·»åŠ ç‰¹å¾
        graph.ndata['feat'] = torch.FloatTensor(node_features)
        graph.ndata['size'] = torch.FloatTensor(node_sizes)

        # æ·»åŠ è¾¹æƒé‡
        edge_weights_bi = edge_weights + edge_weights
        graph.edata['weight'] = torch.FloatTensor(edge_weights_bi)

        return graph, node_ids, np.array(node_sizes)

    def _inject_node_failure(
        self,
        nodes: List[Dict],
        failure_ids: List[str],
        failure_ratio: float
    ) -> List[Dict]:
        """
        æ³¨å…¥èŠ‚ç‚¹ç ´äº§åœºæ™¯

        æ¨¡æ‹ŸèŠ‚ç‚¹å¤±è´¥ï¼š
        - å°†æŒ‡å®šèŠ‚ç‚¹çš„ size å‡å°‘åˆ°åŸæ¥çš„ (1 - failure_ratio)
        - composition ä¸­çš„èµ„äº§æŒ‰æ¯”ä¾‹å‡å°‘
        """
        nodes_copy = [node.copy() for node in nodes]

        for node in nodes_copy:
            if str(node['id']) in failure_ids:
                # ç ´äº§ï¼èµ„äº§å¤§å¹…ç¼©æ°´
                node['size'] *= (1 - failure_ratio)

                # composition ä¹ŸæŒ‰æ¯”ä¾‹ç¼©å‡
                if node['composition']:
                    node['composition'] = {
                        k: v * (1 - failure_ratio)
                        for k, v in node['composition'].items()
                    }

                print(f"  ğŸ’¥ æ³¨å…¥ç ´äº§: èŠ‚ç‚¹ {node['id']} èµ„äº§æŸå¤± {failure_ratio*100:.0f}%")

        return nodes_copy

    def _compute_labels(
        self,
        sizes_t: np.ndarray,
        sizes_t1: np.ndarray
    ) -> torch.Tensor:
        """
        è®¡ç®—å›å½’æ ‡ç­¾

        Returns:
            labels: (num_nodes, 4)
                - [:, 0]: log TVL å˜åŒ– (æ¨èä¸»ç›®æ ‡ï¼Œå¯¹ç§°ä¸”ç¨³å®š)
                - [:, 1]: ç›¸å¯¹å˜åŒ–ç‡ (è£å‰ªåˆ° [-2, 10])
                - [:, 2]: ç»å¯¹æŸå¤± (log scale)
                - [:, 3]: å—å½±å“ç¨‹åº¦ [0, 1]
        """
        # 0. Log TVL å˜åŒ– (ä¸»ç›®æ ‡ - å¯¹ç§°ä¸”ç»Ÿè®¡æ€§è´¨å¥½)
        # y = log1p(size_t+1) - log1p(size_t)
        log_change = np.log1p(np.maximum(sizes_t1, 0)) - np.log1p(np.maximum(sizes_t, 0))
        # è£å‰ªæç«¯å€¼åˆ° [-5, 5] (å¯¹åº”çº¦ -99% åˆ° +14700% çš„å˜åŒ–)
        log_change = np.clip(log_change, -5.0, 5.0)

        # é¿å…é™¤é›¶ï¼šä½¿ç”¨æ›´å®‰å…¨çš„é˜ˆå€¼
        MIN_SIZE = 1000.0  # æœ€å°è§„æ¨¡ $1000
        sizes_t_safe = np.maximum(sizes_t, MIN_SIZE)

        # 1. ç›¸å¯¹å˜åŒ–ç‡ - è£å‰ªæç«¯å€¼
        delta_ratio = (sizes_t1 - sizes_t) / sizes_t_safe
        # è£å‰ªåˆ°åˆç†èŒƒå›´: -200% ~ +1000%
        delta_ratio = np.clip(delta_ratio, -2.0, 10.0)

        # 2. ç»å¯¹æŸå¤±ï¼ˆå–è´Ÿå€¼çš„ logï¼‰
        abs_loss = sizes_t - sizes_t1
        log_abs_loss = np.log1p(np.maximum(abs_loss, 0))

        # 3. å—å½±å“ç¨‹åº¦ [0, 1]
        # å¦‚æœæŸå¤±è¶…è¿‡50%ï¼Œç®—ä¸¥é‡å—å½±å“
        impact_score = np.clip(-delta_ratio, 0, 1)

        labels = np.stack([log_change, delta_ratio, log_abs_loss, impact_score], axis=1)
        return torch.FloatTensor(labels)

    def get_node_info(self, node_id: str) -> Dict:
        """è·å–èŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯"""
        meta = self.meta_dict.get(str(node_id), {})
        return {
            'id': node_id,
            'name': meta.get('name', 'Unknown'),
            'category': meta.get('category', 'Unknown')
        }

    def get_all_temporal_pairs(
        self,
        inject_failure: Optional[List[str]] = None
    ) -> List[Tuple]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ—¶åºå›¾å¯¹"""
        pairs = []
        num_pairs = len(self.dates) - self.prediction_window

        print(f"\næ„å»º {num_pairs} ä¸ªæ—¶åºå›¾å¯¹...")
        for t in range(num_pairs):
            try:
                pair = self.get_temporal_pair(t, inject_failure=inject_failure)
                pairs.append(pair)
            except Exception as e:
                print(f"  âš ï¸  è·³è¿‡ç¬¬ {t} ä¸ªå›¾å¯¹: {e}")

        print(f"âœ“ æˆåŠŸæ„å»º {len(pairs)} ä¸ªå›¾å¯¹\n")
        return pairs

    def _check_data_files(self, auto_download: bool = True):
        """
        æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨,å¦‚æœä¸å­˜åœ¨åˆ™æç¤ºä¸‹è½½æˆ–è‡ªåŠ¨ä¸‹è½½

        Args:
            auto_download: æ˜¯å¦è‡ªåŠ¨è¿è¡Œä¸‹è½½è„šæœ¬
        """
        missing_files = []

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        if not self.data_path.exists():
            missing_files.append(str(self.data_path))

        if not self.meta_path.exists():
            missing_files.append(str(self.meta_path))

        if not missing_files:
            return  # æ‰€æœ‰æ–‡ä»¶éƒ½å­˜åœ¨

        # æœ‰æ–‡ä»¶ç¼ºå¤±
        print("\n" + "="*60)
        print("âŒ æ•°æ®æ–‡ä»¶ç¼ºå¤±!")
        print("="*60)
        for f in missing_files:
            print(f"  - {f}")

        print("\næ•°æ®é›†å¤§å°çº¦ 1.2GB,è¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š\n")

        if auto_download:
            # è‡ªåŠ¨ä¸‹è½½
            print("æ­£åœ¨å°è¯•è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†...\n")

            # è·å–æ•°æ®ç›®å½•
            data_dir = self.data_path.parent

            # è°ƒç”¨ä¸‹è½½è„šæœ¬
            try:
                script_path = Path(__file__).parent.parent.parent / "bin" / "download_dataset.py"
                result = subprocess.run(
                    [sys.executable, str(script_path), "--data-dir", str(data_dir)],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    print("\nâœ“ æ•°æ®é›†ä¸‹è½½æˆåŠŸ!")
                else:
                    print(f"\nâœ— è‡ªåŠ¨ä¸‹è½½å¤±è´¥:")
                    print(result.stderr)
                    print("\nè¯·æ‰‹åŠ¨è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ•°æ®é›†:")
                    print(f"  python {script_path} --data-dir {data_dir}")
                    sys.exit(1)
            except Exception as e:
                print(f"\nâœ— è‡ªåŠ¨ä¸‹è½½å‡ºé”™: {e}")
                print("\nè¯·æ‰‹åŠ¨è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ•°æ®é›†:")
                script_path = Path(__file__).parent.parent.parent / "bin" / "download_dataset.py"
                print(f"  python {script_path} --data-dir {data_dir}")
                sys.exit(1)
        else:
            # ä¸è‡ªåŠ¨ä¸‹è½½,åªæç¤º
            script_path = Path(__file__).parent.parent.parent / "bin" / "download_dataset.py"
            print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ•°æ®é›†:")
            print(f"  python {script_path} --data-dir {self.data_path.parent}\n")
            sys.exit(1)


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

if __name__ == "__main__":
    loader = DeXposureTemporalLoader(
        data_path="/home/figurich/inter-protocol-exposure/DeXposure/data/historical-network_week_2025-07-01.json",
        meta_path="/home/figurich/inter-protocol-exposure/DeXposure/data/meta_df.csv",
        prediction_window=1
    )

    # ç¤ºä¾‹1ï¼šæ­£å¸¸æƒ…å†µä¸‹çš„é¢„æµ‹
    print("=" * 60)
    print("ç¤ºä¾‹1: æ­£å¸¸æƒ…å†µ")
    print("=" * 60)
    graph_t, graph_t1, labels = loader.get_temporal_pair(t=0)
    print(f"å›¾ t:   {graph_t.num_nodes()} èŠ‚ç‚¹, {graph_t.num_edges()} è¾¹")
    print(f"å›¾ t+1: {graph_t1.num_nodes()} èŠ‚ç‚¹, {graph_t1.num_edges()} è¾¹")
    print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    print(f"å¹³å‡å˜åŒ–ç‡: {labels[:, 0].mean():.4f}")
    print(f"å—å½±å“èŠ‚ç‚¹æ•° (>10%): {(labels[:, 2] > 0.1).sum()}")

    # ç¤ºä¾‹2ï¼šæ¨¡æ‹Ÿ Binance ç ´äº§
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹2: æ¨¡æ‹Ÿ Binance (2269) ç ´äº§")
    print("=" * 60)
    binance_info = loader.get_node_info('2269')
    print(f"ç ´äº§èŠ‚ç‚¹: {binance_info}")

    graph_t, graph_t1, labels = loader.get_temporal_pair(
        t=0,
        inject_failure=['2269'],  # Binance
        failure_ratio=0.95  # æŸå¤±95%èµ„äº§
    )

    print(f"\né¢„æµ‹ç»“æœ:")
    print(f"  å—ä¸¥é‡å½±å“çš„èŠ‚ç‚¹æ•° (>50%): {(labels[:, 2] > 0.5).sum()}")
    print(f"  æœ€å¤§æŸå¤±ç‡: {labels[:, 0].min():.2%}")
    print(f"  å¹³å‡å—å½±å“ç¨‹åº¦: {labels[:, 2].mean():.4f}")

    # æ‰¾å‡ºæœ€å—å½±å“çš„å‰10ä¸ªèŠ‚ç‚¹
    top_affected = torch.argsort(labels[:, 2], descending=True)[:10]
    print(f"\næœ€å—å½±å“çš„å‰10ä¸ªèŠ‚ç‚¹:")
    for i, idx in enumerate(top_affected):
        # è¿™é‡Œéœ€è¦ä» graph è·å–èŠ‚ç‚¹IDï¼Œç®€åŒ–èµ·è§å…ˆæ‰“å°ç´¢å¼•
        impact = labels[idx, 2].item()
        loss_ratio = labels[idx, 0].item()
        print(f"  {i+1}. èŠ‚ç‚¹ #{idx}: å—å½±å“ç¨‹åº¦={impact:.2%}, æŸå¤±ç‡={loss_ratio:.2%}")
